{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tr\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import TrajectoryDataset\n",
    "from lightly.models.modules.heads import VICRegProjectionHead\n",
    "from utils import save_model, compute_mean_and_std, get_byol_transforms, get_vicreg_loss\n",
    "from utils import criterion as VICReg_criterion\n",
    "from tqdm import tqdm\n",
    "from models import JEPAModel\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 1024\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "use_expander = False\n",
    "batch_size = 16 \n",
    "\n",
    "dataset_directory = \"./dataset\"\n",
    "states_filename = \"states.npy\"\n",
    "actions_filename = \"actions.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint(model, dataloader, criterion_encoder, criterion_pred, \n",
    "                optimizer, transformation1, transformation2, \n",
    "                device, epochs=10, use_expander=False):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    # clipping the gradient to handle gradient explosions in LSTM\n",
    "    max_val = 5.0\n",
    "    for param in model.predictor.parameters():\n",
    "        if param.grad is not None:\n",
    "            param.grad.data = torch.clamp(param.grad.data, -max_val, max_val)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Processing Batch\"):\n",
    "            state, action = batch\n",
    "            state, action = state.to(device), action.to(device)\n",
    "            B, L, D = state.shape[0], action.shape[1], model.predictor.hidden_size\n",
    "\n",
    "            loss, loss1, loss2, loss3 = 0, 0, 0, 0\n",
    "\n",
    "            o = state[:, 0, :, :, :]\n",
    "            c0 = torch.zeros((B, D)).to(device)\n",
    "            model.set_predictor(o, c0, use_expander)\n",
    "\n",
    "            # compute loss1\n",
    "            loss1 = get_vicreg_loss(model, o, transformation1, transformation2, \n",
    "                                     criterion_encoder)\n",
    "            for i in range(L):\n",
    "                # inference of encoder(next state) and predictor(action) \n",
    "                sy_hat, (sy_enc, sy_exp) = model(action[:, i, :], state[:, i+1, :, :, :])\n",
    "                sy = sy_exp if use_expander else sy_enc\n",
    "\n",
    "                # compute loss2 (distance btw sy and sy_hat)\n",
    "                loss2 += criterion_pred(sy_hat, sy)\n",
    "                # vic_reg loss for encoder (for encoding next state)\n",
    "                loss3 += get_vicreg_loss(model, state[:, i, :, :, :], \n",
    "                                          transformation1, transformation2, \n",
    "                                          criterion_encoder) \n",
    "            \n",
    "            # adding all loss and doing back propagation\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, total_loss: {total_loss}, the avg loss = {total_loss/len(dataloader)}\")\n",
    "        save_model(model, epoch, file_name=\"join_model\")\n",
    "\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrajectoryDataset(\n",
    "    data_dir = dataset_directory,\n",
    "    states_filename = states_filename,\n",
    "    actions_filename = actions_filename,\n",
    "    s_transform = None,\n",
    "    a_transform = None,\n",
    "    length = 1000 \n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "first_datapoint = next(iter(dataloader))\n",
    "state, action = first_datapoint\n",
    "print(f\"Number of data_points {len(dataloader)}\")\n",
    "print(f\"Shape of state: {state.shape}\")\n",
    "print(f\"Shape of action: {action.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = compute_mean_and_std(dataloader, is_channelsize3=False)\n",
    "transformation1, transformation2 = get_byol_transforms(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JEPAModel(embed_dim, 2)\n",
    "\n",
    "joint_optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1.5e-4)\n",
    "# joint_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_predictor = nn.MSELoss()\n",
    "criterion_encoder = VICReg_criterion\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_joint(model, dataloader, criterion_encoder, criterion_predictor, \n",
    "            joint_optimizer, transformation1, transformation2, device,\n",
    "            epochs=epochs, use_expander=use_expander)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
